{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import nltk as nl\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_curve\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import keras.backend as K\n",
    "K.set_floatx('float32')\n",
    "porter_stemmer = PorterStemmer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indo_data = pd.read_csv('./Dataset/spam_indo.csv', delimiter=',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([indo_data], ignore_index=True)\n",
    "df = df.rename(columns={'type': 'label', 'text': 'message'})\n",
    "\n",
    "df = pd.concat([indo_data.rename(columns={'type': 'label', 'text': 'message'})], ignore_index=True)\n",
    "df.head()\n",
    "print(df.describe())  # combined datas\n",
    "check = 'ham'\n",
    "print(\"Ham data count:\")\n",
    "print(df[df['label'] == check].count().get(0))\n",
    "check = 'spam'\n",
    "print(\"Spam data count:\")\n",
    "print(df[df['label'] == check].count().get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = indo_data.copy()\n",
    "real_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = real_data.rename(columns={'type': 'label', 'text': 'message'})\n",
    "real_data.head()\n",
    "print(\"Real Data:\")\n",
    "print(real_data.describe())\n",
    "print(real_data[real_data['label'] == 'ham'].count().get(0))\n",
    "print(real_data[real_data['label'] == 'spam'].count().get(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nl.corpus.stopwords.words('english')\n",
    "gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "sklearn_stopwords = _stop_words.ENGLISH_STOP_WORDS\n",
    "combined_stopwords = sklearn_stopwords.union(nltk_stopwords, gensim_stopwords)\n",
    "# preprocessing on sms_dataset\n",
    "real_data['message'] = real_data['message'].apply(lambda x: x.lower())\n",
    "real_data['message'] = real_data['message'].str.replace('[^\\w\\s]', '')\n",
    "real_data['message'] = real_data['message'].apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in (combined_stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = real_data.message\n",
    "Y = real_data.label\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "X_transform = sequence.pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_transform, Y, test_size=test_ratio)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train,\n",
    "                                                  test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "                                                      random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_len))\n",
    "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Melatih model\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# Simpan model ke file .h5\n",
    "model.save(\"spam_classifier_model.h5\")\n",
    "\n",
    "# Membuat prediksi\n",
    "Y_pred_probs = model.predict(X_test)\n",
    "Y_pred = (Y_pred_probs > 0.5).astype('int32')  # Menggunakan threshold 0.5 untuk klasifikasi biner\n",
    "\n",
    "# Atau Anda juga bisa menggunakan numpy untuk menerapkan threshold dengan lebih fleksibel\n",
    "# import numpy as np\n",
    "# threshold = 0.5\n",
    "# Y_pred = np.where(Y_pred_probs > threshold, 1, 0)  # Menggunakan threshold sesuai kebutuhan Anda\n",
    "\n",
    "# print(classification_report(Y_test, Y_pred, target_names=['Ham', 'Spam']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'tok' is your tokenizer object\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred, target_names=['Ham', 'Spam']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(Y_test, Y_pred), columns=['Predicted Ham', 'Predicted Spam'], index=['Ham', 'Spam']))\n",
    "print(f'Accuracy: {round(accuracy_score(Y_test, Y_pred), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heatmap = sns.heatmap(data=pd.DataFrame(confusion_matrix(Y_test, Y_pred)), annot=True, fmt=\"d\", cmap=sns.color_palette(\"Blues\", 50))\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "plt.ylabel('Ground Truth for LSTM')\n",
    "plt.xlabel('Prediction for LSTM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained LSTM model and tokenizer\n",
    "model = load_model('spam_classifier_model.h5')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Function to preprocess text and make predictions\n",
    "def classify_message(message):\n",
    "    max_len = 150  # Same as during training\n",
    "    sequence = tokenizer.texts_to_sequences([message])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return prediction\n",
    "\n",
    "# Streamlit app\n",
    "st.title('Spam or Ham Classifier')\n",
    "user_input = st.text_area(\"Enter the message you'd like to classify:\")\n",
    "\n",
    "if st.button('Classify'):\n",
    "    prediction = classify_message(user_input)\n",
    "    if prediction > 0.5:\n",
    "        st.write(\"The message is likely to be Spam.\")\n",
    "    else:\n",
    "        st.write(\"The message is likely to be Ham.\")\n",
    "    \n",
    "    # Displaying the prediction confidence\n",
    "    st.write(f\"Confidence: {np.max(prediction) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
